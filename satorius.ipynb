{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Imports ","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:12.980644Z","iopub.execute_input":"2021-12-21T02:08:12.980933Z","iopub.status.idle":"2021-12-21T02:08:15.579646Z","shell.execute_reply.started":"2021-12-21T02:08:12.980841Z","shell.execute_reply":"2021-12-21T02:08:15.578881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        \nrandom_seed(1702)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.581101Z","iopub.execute_input":"2021-12-21T02:08:15.58136Z","iopub.status.idle":"2021-12-21T02:08:15.631688Z","shell.execute_reply.started":"2021-12-21T02:08:15.581327Z","shell.execute_reply":"2021-12-21T02:08:15.631022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nval_img_dir = \"../input/sartorius-cell-instance-segmentation/test\"\ntrain_img_dir = \"../input/sartorius-cell-instance-segmentation/train\"\ntrain_csv_dir = \"../input/sartorius-cell-instance-segmentation/train.csv\"\n\nwidth = 704\nheight = 520\nn_epochs = 30\nbatch_size = 2\nmomentum = 0.9\nlearning_rate = 0.001\nweight_decay = 0.0005 \nnum_box_detections = 540\n\nmin_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\ncell_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\nmask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6}\n\nNORMALIZE = True\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.634178Z","iopub.execute_input":"2021-12-21T02:08:15.634454Z","iopub.status.idle":"2021-12-21T02:08:15.641708Z","shell.execute_reply.started":"2021-12-21T02:08:15.63442Z","shell.execute_reply":"2021-12-21T02:08:15.640866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Helpers \n\nI don't write any code in this section except GaussianBlur. \n\n#### References: \n- https://www.kaggle.com/inversion/run-length-decoding-quick-start\n- https://www.kaggle.com/theoviel/competition-metric-map-iou\n- https://www.kaggle.com/abhishek/maskrcnn-utils","metadata":{}},{"cell_type":"code","source":"# Ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height, width, channels) of array to return\n    color: color for the mask\n    Returns numpy array (mask)\n\n    '''\n    s = mask_rle.split()\n\n    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n    lengths = list(map(int, s[1::2]))\n    ends = [x + y for x, y in zip(starts, lengths)]\n    if len(shape)==3:\n        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    else:\n        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for start, end in zip(starts, ends):\n        img[start : end] = color\n\n    return img.reshape(shape)\n\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask\n\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((height, width))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n        maskimg[mask>mask_threshold] = m\n    return maskimg\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    for i, mask in enumerate(pred[\"masks\"]):\n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n\n    return use_masks","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.644303Z","iopub.execute_input":"2021-12-21T02:08:15.644577Z","iopub.status.idle":"2021-12-21T02:08:15.661677Z","shell.execute_reply.started":"2021-12-21T02:08:15.644541Z","shell.execute_reply":"2021-12-21T02:08:15.661055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ref: https://www.kaggle.com/theoviel/competition-metric-map-iou\n        \ndef compute_iou(labels, y_pred, verbose=0):\n    \"\"\"\n    Computes the IoU for instance labels and predictions.\n\n    Args:\n        labels (np array): Labels.\n        y_pred (np array): predictions\n\n    Returns:\n        np array: IoU matrix, of size true_objects x pred_objects.\n    \"\"\"\n\n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    if verbose:\n        print(\"Number of true objects: {}\".format(true_objects))\n        print(\"Number of predicted objects: {}\".format(pred_objects))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(\n        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n    )[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    intersection = intersection[1:, 1:] # exclude background\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n    iou = intersection / union\n    \n    return iou  \n\ndef precision_at(threshold, iou):\n    \"\"\"\n    Computes the precision at a given threshold.\n\n    Args:\n        threshold (float): Threshold.\n        iou (np array): IoU matrix.\n\n    Returns:\n        int: Number of true positives,\n        int: Number of false positives,\n        int: Number of false negatives.\n    \"\"\"\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    tp, fp, fn = (\n        np.sum(true_positives),\n        np.sum(false_positives),\n        np.sum(false_negatives),\n    )\n    return tp, fp, fn\n\ndef iou_map(truths, preds, verbose=0):\n    \"\"\"\n    Computes the metric for the competition.\n    Masks contain the segmented pixels where each object has one value associated,\n    and 0 is the background.\n\n    Args:\n        truths (list of masks): Ground truths.\n        preds (list of masks): Predictions.\n        verbose (int, optional): Whether to print infos. Defaults to 0.\n\n    Returns:\n        float: mAP.\n    \"\"\"\n    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n\n    if verbose:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for iou in ious:\n            tp, fp, fn = precision_at(t, iou)\n            tps += tp\n            fps += fp\n            fns += fn\n\n        p = tps / (tps + fps + fns)\n        prec.append(p)\n\n        if verbose:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n\n    if verbose:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n\n    return np.mean(prec)\n\n\ndef get_score(ds, mdl):\n    \"\"\"\n    Get average IOU mAP score for a dataset\n    \"\"\"\n    mdl.eval()\n    iouscore = 0\n    for i in tqdm(range(len(ds))):\n        img, targets = ds[i]\n        with torch.no_grad():\n            result = mdl([img.to(DEVICE)])[0]\n            \n        masks = combine_masks(targets['masks'], 0.5)\n        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n\n        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n        iouscore += iou_map([masks],[pred_masks])\n    return iouscore / len(ds)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.663269Z","iopub.execute_input":"2021-12-21T02:08:15.663674Z","iopub.status.idle":"2021-12-21T02:08:15.684153Z","shell.execute_reply.started":"2021-12-21T02:08:15.663637Z","shell.execute_reply":"2021-12-21T02:08:15.683193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ref https://www.kaggle.com/abhishek/maskrcnn-utils\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\nclass GaussianBlur:\n    def __init__(self, prob):\n        self.prob = prob\n    \n    def __call__(self, image, target): \n        if random.random() < self.prob:\n            image = F.gaussian_blur(image, kernel_size=(5, 9), sigma=(0.1, 1.5))\n        \n        return image, target\n\ndef get_transform(train):\n    transforms = [ToTensor()]\n    if NORMALIZE:\n        transforms.append(Normalize())\n    \n    # Data augmentation for train\n    if train: \n        transforms.append(HorizontalFlip(0.5))\n        transforms.append(VerticalFlip(0.5))\n        transforms.append(GaussianBlur(0.2))\n\n    return Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.686783Z","iopub.execute_input":"2021-12-21T02:08:15.687499Z","iopub.status.idle":"2021-12-21T02:08:15.704407Z","shell.execute_reply.started":"2021-12-21T02:08:15.687291Z","shell.execute_reply":"2021-12-21T02:08:15.703599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Dataset","metadata":{}},{"cell_type":"markdown","source":"### a. Dataset","metadata":{}},{"cell_type":"code","source":"class SatoriusDataset(Dataset): \n    def __init__(self, csv_file, image_dir, transform=None, resize=None):\n        self.transform = transform\n        self.csv = csv_file \n        self.resize = True if resize else False \n        \n        # resize is the factor, e.g 1.2, 1.3, ... \n        if self.resize: \n            self.height = int(height * resize)\n            self.width = int(width * resize)\n        else: \n            self.height = height\n            self.width = width\n        \n        # collections.defaultdict returns an empty dict when the key is not found\n        self.image_info = collections.defaultdict(dict)\n        \n        # group by id then cell, and get the annotations \n        csv_groupby = self.csv.groupby(['id', 'cell_type'])['annotation'].agg(lambda x: list(x)).reset_index()\n        \n        \n        # image_info is the replacement of csv \n        # use image_info to iterate\n        for index, row in csv_groupby.iterrows(): \n            self.image_info[index] = { \n                'image_id': row['id'],\n                'image_path': os.path.join(image_dir, row['id'] + '.png'),\n                'annotations': row['annotation'],\n                'cell_type': cell_dict[row['cell_type']],\n            }        \n        \n    def __len__(self): \n        return len(self.image_info)\n\n    def get_boxes(self, mask):\n        # the size of the mask is [w, h] and the values are True and False\n        # using np.where will return the location of the True value in the mask \n        mask = np.where(mask)\n\n        # locations of the bounding box \n        x_min = np.min(mask[1])\n        x_max = np.max(mask[1])\n        y_min = np.min(mask[0])\n        y_max = np.max(mask[0])\n\n        return [x_min, y_min, x_max, y_max]\n\n    def __getitem__(self, idx): \n        item = self.image_info[idx]\n\n        image = cv2.imread(item['image_path'], cv2.IMREAD_COLOR)        \n        if self.resize: \n            image = cv2.resize(image, (self.width, self.height))\n        \n        # height goes before width because of rle_decode()\n        masks = np.zeros((len(item['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n\n        for i, mask in enumerate(item['annotations']): \n            mask = rle_decode(mask, (height, width))\n\n            if self.resize: \n                mask = cv2.resize(mask, (self.width, self.height))\n                \n            # the mask only contains true and false, which are 1 and 0\n            mask = np.array(mask) > 0\n            masks[i,:,:] = mask\n            boxes.append(self.get_boxes(mask))\n\n        labels = [item['cell_type'] for _ in range(len(item['annotations']))]\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        \n        # targets must have for the mask-rcnn\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"masks\": masks,\n        }\n\n        if self.transform: \n            image, target = self.transform(image, target)\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.706822Z","iopub.execute_input":"2021-12-21T02:08:15.707503Z","iopub.status.idle":"2021-12-21T02:08:15.725894Z","shell.execute_reply.started":"2021-12-21T02:08:15.707465Z","shell.execute_reply":"2021-12-21T02:08:15.725162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. Plot the dataset","metadata":{}},{"cell_type":"code","source":"test_dataset_csv = pd.read_csv(train_csv_dir)\ntest_dataset = SatoriusDataset(test_dataset_csv, train_img_dir)\n\ntest = test_dataset.__getitem__(1)\nimg = test[0]\ntarget = test[1]\n\nboxes = target['boxes']\nmasks = target['masks']\nlabels = target['labels']\n\nprint(f\"img  :  {img.shape}\")\nprint(f\"boxes:  {boxes.shape}\")\nprint(f\"labels: {labels.shape}\")\nprint(f\"masks:  {masks.shape}\")\n\nprint(\"labels:   \", labels.unique())\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\nax[0].imshow(img)\nax[1].imshow(combine_masks(masks, 0))\n\nfor box in boxes:\n    # box[1] - box[3] because y-axis is reversed\n    ax[1].add_patch(patches.Rectangle((box[0], box[3]), box[2] - box[0], box[1] - box[3], linewidth=1, edgecolor='r', facecolor='none'))\n    \nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:15.727403Z","iopub.execute_input":"2021-12-21T02:08:15.72792Z","iopub.status.idle":"2021-12-21T02:08:17.188711Z","shell.execute_reply.started":"2021-12-21T02:08:15.727885Z","shell.execute_reply":"2021-12-21T02:08:17.187981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Model ","metadata":{}},{"cell_type":"code","source":"class MaskRCNN(nn.Module):\n    def __init__(self, num_classes, pretrained_model=None):\n        super().__init__()\n        \n        self.pretrained_model = pretrained_model\n        \n        if NORMALIZE: \n            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                      box_detections_per_img=num_box_detections, \n                                                                      image_mean=RESNET_MEAN, \n                                                                      image_std=RESNET_STD)\n        else:\n            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                      box_detections_per_img=num_box_detections)\n\n        hidden_layer = 256\n        \n        # get the input features of the box_predictor \n        # replace the pretrained box_predictor layer with new FastRCNN\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n        \n        # get the input features of the mask_predictor \n        # replace the pretrained mask_predictor layer with new MaskRCNN\n        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n        self.model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n                \n    def forward(self, x, y=None): \n        # in the training mode, the model needs both x and y (y is the target)\n        # in the val mode, the model only needs x\n        if y: \n            return self.model(x, y)\n        \n        return self.model(x)\n    \n#     def init_model(self):\n#         if self.pretrained_model:\n#             self.model.load_state_dict(self.pretrained_model, map_location=DEVICE)\n        \n#         print(\"Loaded pretrained weights\")","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:17.189748Z","iopub.execute_input":"2021-12-21T02:08:17.189987Z","iopub.status.idle":"2021-12-21T02:08:17.199921Z","shell.execute_reply.started":"2021-12-21T02:08:17.189954Z","shell.execute_reply":"2021-12-21T02:08:17.198852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Training","metadata":{}},{"cell_type":"markdown","source":"### a. Trainer","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, optimizer, device, scheduler):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = DEVICE\n        \n        self.train_loss_mask = []\n        self.train_loss_classifier = []\n        self.train_loss = []\n        self.val_loss_mask = []\n        self.val_loss_classifier = []\n        self.val_loss = []\n        \n    def fit(self, train_loader, val_loader, n_epochs):\n        for epoch in range(n_epochs): \n            \n            # train and val\n            self.train_epoch(train_loader)            \n            self.val_epoch(val_loader)\n            \n            if self.scheduler: \n                self.scheduler.step() \n            \n            print(f\"epoch {epoch + 1} train:  mask loss : {self.train_loss_mask[-1]:3f}, classifier loss:  {self.train_loss_classifier[-1]:3f}\")\n            print(f\"epoch {epoch + 1} val:    mask loss : {self.val_loss_mask[-1]:3f},   classifier loss: {self.val_loss_classifier[-1]:3f}\")\n            print(f\"epoch {epoch + 1}:        train loss: {self.train_loss[-1]:3f},      val_loss       : {self.val_loss[-1]:3f}\")\n\n            print('save model ...')\n            torch.save(self.model.state_dict(), f'model-e{epoch + 1}.pt')\n\n            print('\\n\\n')\n                                \n    def train_epoch(self, train_loader):\n        self.model.train()\n        self.train_loss_mask.append(0)\n        self.train_loss_classifier.append(0)\n        self.train_loss.append(0)\n\n        for i, data in tqdm(enumerate(train_loader, 0)): \n            x, y = data\n            x = list(img.to(self.device) for img in x)\n            y = [{k: v.to(self.device) for k, v in t.items()} for t in y]\n            \n            self.optimizer.zero_grad()\n            \n            # in training mode, the model return the loss\n            # in val mode, the model return the predictions\n            losses = self.model(x, y)\n            loss = sum(loss for loss in losses.values())\n            loss.backward()\n            self.optimizer.step()\n            \n            self.train_loss_mask[-1] += losses['loss_mask'].item()\n            self.train_loss_classifier[-1] += losses['loss_classifier'].item()\n            self.train_loss[-1] += loss.item()\n        \n        # gets the average\n        self.train_loss_mask[-1] /= len(train_loader)\n        self.train_loss_classifier[-1] /= len(train_loader)\n        self.train_loss[-1] /= len(train_loader)\n        \n    def val_epoch(self, val_loader):\n        self.val_loss_mask.append(0)\n        self.val_loss_classifier.append(0)\n        self.val_loss.append(0)\n\n        for i, data in tqdm(enumerate(val_loader, 0)): \n            with torch.no_grad():\n                x, y = data \n                x = list(img.to(self.device) for img in x)\n                y = [{k: v.to(self.device) for k, v in t.items()} for t in y]\n                \n                # in training mode, the model return the loss\n                # in val mode, the model return the predictions\n                losses = model(x, y)\n                loss = sum(loss for loss in losses.values())\n                \n                self.val_loss_mask[-1] += losses['loss_mask'].item()\n                self.val_loss_classifier[-1] += losses['loss_classifier'].item()\n                self.val_loss[-1] += loss.item()\n\n        # gets the average\n        self.val_loss_mask[-1] /= len(val_loader)\n        self.val_loss_classifier[-1] /= len(val_loader)\n        self.val_loss[-1] /= len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:17.203716Z","iopub.execute_input":"2021-12-21T02:08:17.204145Z","iopub.status.idle":"2021-12-21T02:08:17.224501Z","shell.execute_reply.started":"2021-12-21T02:08:17.204082Z","shell.execute_reply":"2021-12-21T02:08:17.223855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. Training","metadata":{}},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/cocopre/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:17.225761Z","iopub.execute_input":"2021-12-21T02:08:17.226021Z","iopub.status.idle":"2021-12-21T02:08:21.736085Z","shell.execute_reply.started":"2021-12-21T02:08:17.225987Z","shell.execute_reply":"2021-12-21T02:08:21.735005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(train_csv_dir)\n\n# the layout of the df_images would be: id, cell_type, annotation\n# there would be one row for one image because each image only contains one cell_type\ndf_images = train_csv.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n\ntrain_df, test_df = train_test_split(df_images, stratify=df_images['cell_type'], \n                                                  test_size=0.2,\n                                                  random_state=1702)\n\n# dataloader for training\ntrain_df = train_csv[train_csv['id'].isin(train_df['id'])]\ntrain_dataset = SatoriusDataset(train_df, train_img_dir, transform=get_transform(train=True))\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                              num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n\n# dataloader for validation\ntest_df = train_csv[train_csv['id'].isin(test_df['id'])]\ntest_dataset = SatoriusDataset(test_df, train_img_dir, transform=get_transform(train=False))\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True,\n                             num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:21.738057Z","iopub.execute_input":"2021-12-21T02:08:21.738595Z","iopub.status.idle":"2021-12-21T02:08:22.156314Z","shell.execute_reply.started":"2021-12-21T02:08:21.738553Z","shell.execute_reply":"2021-12-21T02:08:22.155565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MaskRCNN(len(cell_dict))\nmodel.to(DEVICE)\n\nfor param in model.parameters():\n    param.requires_grad = True\n    \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n\ntrainer = Trainer(model, optimizer, DEVICE, lr_scheduler)\ntrainer.fit(train_dataloader, test_dataloader, n_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:22.157754Z","iopub.execute_input":"2021-12-21T02:08:22.158033Z","iopub.status.idle":"2021-12-21T02:08:22.162671Z","shell.execute_reply.started":"2021-12-21T02:08:22.157997Z","shell.execute_reply":"2021-12-21T02:08:22.161684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. Plot the loss","metadata":{}},{"cell_type":"code","source":"train_loss_mask = trainer.train_loss_mask\ntrain_loss_classifier = trainer.train_loss_classifier\ntrain_loss = trainer.train_loss\nval_loss_mask = trainer.val_loss_mask\nval_loss_classifier = trainer.val_loss_classifier\nval_loss = trainer.val_loss\n\nfig, ax = plt.subplots(1, 3, figsize=(17, 5))\nax[0].plot(train_loss_mask, color='red', label=\"train\")\nax[0].plot(val_loss_mask, color='blue', label=\"val\")\nax[1].plot(train_loss_classifier, color='red', label=\"train\")\nax[1].plot(val_loss_classifier, color='blue', label=\"val\")\nax[2].plot(train_loss, color='red', label=\"train\")\nax[2].plot(val_loss, color='blue', label=\"val\")\n\nax[0].title.set_text(\"mask_loss\")\nax[1].title.set_text(\"classifier_loss\")\nax[2].title.set_text(\"total_loss\")\n\nax[0].legend()\nax[1].legend()\nax[2].legend()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:22.164055Z","iopub.execute_input":"2021-12-21T02:08:22.164339Z","iopub.status.idle":"2021-12-21T02:08:22.173092Z","shell.execute_reply.started":"2021-12-21T02:08:22.164305Z","shell.execute_reply":"2021-12-21T02:08:22.172392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### d. Get the best model based on IOU mAP","metadata":{}},{"cell_type":"code","source":"scores = pd.DataFrame()\n\nfor e in range(n_epochs): \n    model = MaskRCNN(len(cell_dict))\n    model.load_state_dict(torch.load(f\"model-e{e + 1}.pt\"))\n    model.to(DEVICE)\n    scores.loc[e, \"score\"] = get_score(test_dataset, model)\n\nbest_model = \"\"\nscores = scores.sort_values(\"score\", ascending=False)\n\nbest_model = f'model-e{np.argmax(scores[\"score\"]) + 1}.pt'\n\nscores","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:22.176246Z","iopub.execute_input":"2021-12-21T02:08:22.176455Z","iopub.status.idle":"2021-12-21T02:08:23.095721Z","shell.execute_reply.started":"2021-12-21T02:08:22.176432Z","shell.execute_reply":"2021-12-21T02:08:23.094782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### e. Load the best model and plot the prediction","metadata":{}},{"cell_type":"code","source":"try: \n    if best_model == \"\":\n        best_model = \"../input/satorius-models/model-e9.pt\"\nexcept: \n    best_model = \"../input/satorius-models/model-e9.pt\"\n    \nmodel = MaskRCNN(len(cell_dict))\nmodel.load_state_dict(torch.load(best_model))\nmodel.to(DEVICE)\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:45.699776Z","iopub.execute_input":"2021-12-21T02:08:45.700391Z","iopub.status.idle":"2021-12-21T02:08:52.199596Z","shell.execute_reply.started":"2021-12-21T02:08:45.700348Z","shell.execute_reply":"2021-12-21T02:08:52.197779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_dataset_temp = SatoriusDataset(test_df, train_img_dir)\n\n# the number of image will be num_rows * 2 since batch = 2\nnum_rows = 3\nfig, ax = plt.subplots(num_rows * batch_size, 3, figsize=(15, 20))\n\nfor i, data in enumerate(test_dataloader): \n    x, y = data\n    x = list(img.to(DEVICE) for img in x)\n    \n    with torch.no_grad():\n            predictions = model(x, None)\n    \n    for j in range(batch_size):\n        original_img = test_dataset_temp.__getitem__(i * 2 + j)[0]\n        \n        # get the label with the highest frequency \n        label = np.array(np.unique(predictions[j][\"labels\"].cpu().numpy(), return_counts=True))\n        label = label[0][np.argmax(label.T[:,1])]\n        \n        thresh_hold = mask_threshold_dict[label]\n        \n        original_mask = combine_masks(y[j][\"masks\"], 0)\n        predicted_mask = combine_masks(predictions[j][\"masks\"].squeeze().cpu().numpy(), thresh_hold)\n        \n        iou_score = iou_map([original_mask],[predicted_mask])\n        \n        ax[i * 2 + j, 0].imshow(original_img)\n        ax[i * 2 + j, 1].imshow(original_mask)\n        ax[i * 2 + j, 2].imshow(predicted_mask)\n        \n        ax[i * 2 + j, 0].axis('off')\n        ax[i * 2 + j, 1].axis('off')\n        ax[i * 2 + j, 2].axis('off')\n\n        ax[i * 2 + j, 1].title.set_text(f'num_cells: {len(y[j][\"masks\"])}')\n        ax[i * 2 + j, 2].title.set_text(f'num_cells: {len(predictions[j][\"masks\"].squeeze().cpu().numpy())}, iou: {round(iou_score, 3)}')\n        \n    if i == num_rows - 1: \n        break","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:15:04.556389Z","iopub.execute_input":"2021-12-21T02:15:04.557207Z","iopub.status.idle":"2021-12-21T02:15:13.202569Z","shell.execute_reply.started":"2021-12-21T02:15:04.557166Z","shell.execute_reply":"2021-12-21T02:15:13.199328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Submission","metadata":{}},{"cell_type":"code","source":"class SatoriusSubmissionDataset(Dataset): \n    def __init__(self, image_dir, transform=None, resize=None):\n        self.transform = transform\n        self.resize = True if resize else False \n        self.image_dir = image_dir\n        self.image_id = [path[:-4] for path in os.listdir(image_dir)]\n        \n        if self.resize: \n            self.height = int(height * resize)\n            self.width = int(width * resize)\n        \n    def __len__(self): \n        return len(self.image_id)\n\n    def __getitem__(self, idx): \n        image_id = self.image_id[idx]\n        image = cv2.imread(os.path.join(self.image_dir, image_id + \".png\"), cv2.IMREAD_COLOR)  \n\n        if self.resize: \n            image = cv2.resize(image, (self.width, self.height))\n        \n        if self.transform: \n            image, _ = self.transform(image, target=None)\n\n        return image, image_id","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:23.100867Z","iopub.status.idle":"2021-12-21T02:08:23.101502Z","shell.execute_reply.started":"2021-12-21T02:08:23.101264Z","shell.execute_reply":"2021-12-21T02:08:23.101288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_dataset = SatoriusSubmissionDataset(val_img_dir, transform=get_transform(train=False))\nsubmission_dataloader = DataLoader(submission_dataset, batch_size=1, pin_memory=True,\n                             num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:23.102689Z","iopub.status.idle":"2021-12-21T02:08:23.103318Z","shell.execute_reply.started":"2021-12-21T02:08:23.103065Z","shell.execute_reply":"2021-12-21T02:08:23.103089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = []\n\nfor _, data in enumerate(submission_dataloader): \n    x, y = data\n    x = list(img.to(DEVICE) for img in x)\n    \n    with torch.no_grad():\n            predictions = model(x)[0]\n    \n    previous_masks = []\n    \n    # get the label with the highes frequency\n    label = np.array(np.unique(predictions[\"labels\"].cpu().numpy(), return_counts=True))\n    label = label[0][np.argmax(label.T[:,1])]\n\n    for i, mask in enumerate(predictions[\"masks\"]):\n        score = predictions[\"scores\"][i].cpu().item()\n        \n        # compare the score with the threshold \n        if score > min_score_dict[label]:\n            mask = mask.cpu().numpy()\n            \n            # compare the mask with the threshold and remove overlapping\n            mask = mask > mask_threshold_dict[label]\n            mask = remove_overlapping_pixels(mask, previous_masks)\n            rle = rle_encoding(mask)\n            \n            previous_masks.append(mask)\n            \n            # the competition requires submission with the location of each cell in a picture as a row\n            submission.append((y[0], rle))\n            \n    # if there is no rle_encoding for an image, simply append emtpy string since there is no mask\n    all_ids = [image_id for image_id, rle in submission]\n    if y[0] not in all_ids:\n        submission.append((y[0], \"\"))\n    \ndf_submission = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_submission.to_csv(\"submission.csv\", index=False)\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:23.104513Z","iopub.status.idle":"2021-12-21T02:08:23.105307Z","shell.execute_reply.started":"2021-12-21T02:08:23.104946Z","shell.execute_reply":"2021-12-21T02:08:23.105049Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
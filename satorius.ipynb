{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Imports ","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.301777Z","iopub.execute_input":"2021-11-19T06:21:52.302138Z","iopub.status.idle":"2021-11-19T06:21:52.309743Z","shell.execute_reply.started":"2021-11-19T06:21:52.302096Z","shell.execute_reply":"2021-11-19T06:21:52.308896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n    \nrandom_seed(1702)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.311534Z","iopub.execute_input":"2021-11-19T06:21:52.312431Z","iopub.status.idle":"2021-11-19T06:21:52.32647Z","shell.execute_reply.started":"2021-11-19T06:21:52.312387Z","shell.execute_reply":"2021-11-19T06:21:52.325782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Configuration","metadata":{}},{"cell_type":"code","source":"width = 704\nheight = 520\ncell_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\nnum_box_detections = 600\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nn_epochs = 10\nbatch_size = 2\nlearning_rate = 0.001\nmomentum = 0.9\nweight_decay = 0.0005 \n\nNORMALIZE = True\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.328345Z","iopub.execute_input":"2021-11-19T06:21:52.328537Z","iopub.status.idle":"2021-11-19T06:21:52.33749Z","shell.execute_reply.started":"2021-11-19T06:21:52.328506Z","shell.execute_reply":"2021-11-19T06:21:52.33523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Helpers ","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height, width, channels) of array to return\n    color: color for the mask\n    Returns numpy array (mask)\n\n    '''\n    s = mask_rle.split()\n\n    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n    lengths = list(map(int, s[1::2]))\n    ends = [x + y for x, y in zip(starts, lengths)]\n    if len(shape)==3:\n        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    else:\n        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for start, end in zip(starts, ends):\n        img[start : end] = color\n\n    return img.reshape(shape)\n\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask\n\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((height, width))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n        maskimg[mask>mask_threshold] = m\n    return maskimg\n\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    for i, mask in enumerate(pred[\"masks\"]):\n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n\n    return use_masks","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.339928Z","iopub.execute_input":"2021-11-19T06:21:52.340206Z","iopub.status.idle":"2021-11-19T06:21:52.356215Z","shell.execute_reply.started":"2021-11-19T06:21:52.340172Z","shell.execute_reply":"2021-11-19T06:21:52.355459Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ref: https://www.kaggle.com/theoviel/competition-metric-map-iou\n        \ndef compute_iou(labels, y_pred, verbose=0):\n    \"\"\"\n    Computes the IoU for instance labels and predictions.\n\n    Args:\n        labels (np array): Labels.\n        y_pred (np array): predictions\n\n    Returns:\n        np array: IoU matrix, of size true_objects x pred_objects.\n    \"\"\"\n\n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    if verbose:\n        print(\"Number of true objects: {}\".format(true_objects))\n        print(\"Number of predicted objects: {}\".format(pred_objects))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(\n        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n    )[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    intersection = intersection[1:, 1:] # exclude background\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n    iou = intersection / union\n    \n    return iou  \n\ndef precision_at(threshold, iou):\n    \"\"\"\n    Computes the precision at a given threshold.\n\n    Args:\n        threshold (float): Threshold.\n        iou (np array): IoU matrix.\n\n    Returns:\n        int: Number of true positives,\n        int: Number of false positives,\n        int: Number of false negatives.\n    \"\"\"\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    tp, fp, fn = (\n        np.sum(true_positives),\n        np.sum(false_positives),\n        np.sum(false_negatives),\n    )\n    return tp, fp, fn\n\ndef iou_map(truths, preds, verbose=0):\n    \"\"\"\n    Computes the metric for the competition.\n    Masks contain the segmented pixels where each object has one value associated,\n    and 0 is the background.\n\n    Args:\n        truths (list of masks): Ground truths.\n        preds (list of masks): Predictions.\n        verbose (int, optional): Whether to print infos. Defaults to 0.\n\n    Returns:\n        float: mAP.\n    \"\"\"\n    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n\n    if verbose:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for iou in ious:\n            tp, fp, fn = precision_at(t, iou)\n            tps += tp\n            fps += fp\n            fns += fn\n\n        p = tps / (tps + fps + fns)\n        prec.append(p)\n\n        if verbose:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n\n    if verbose:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n\n    return np.mean(prec)\n\n\ndef get_score(ds, mdl):\n    \"\"\"\n    Get average IOU mAP score for a dataset\n    \"\"\"\n    mdl.eval()\n    iouscore = 0\n    for i in tqdm(range(len(ds))):\n        img, targets = ds[i]\n        with torch.no_grad():\n            result = mdl([img.to(DEVICE)])[0]\n            \n        masks = combine_masks(targets['masks'], 0.5)\n        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n\n        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n        iouscore += iou_map([masks],[pred_masks])\n    return iouscore / len(ds)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.359542Z","iopub.execute_input":"2021-11-19T06:21:52.359817Z","iopub.status.idle":"2021-11-19T06:21:52.379132Z","shell.execute_reply.started":"2021-11-19T06:21:52.359767Z","shell.execute_reply":"2021-11-19T06:21:52.378412Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Dataloader","metadata":{}},{"cell_type":"code","source":"# Ref https://www.kaggle.com/abhishek/maskrcnn-utils\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n    \n\ndef get_transform(train):\n    transforms = [ToTensor()]\n    if NORMALIZE:\n        transforms.append(Normalize())\n    \n    # Data augmentation for train\n    if train: \n        transforms.append(HorizontalFlip(0.5))\n        transforms.append(VerticalFlip(0.5))\n\n    return Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.395065Z","iopub.execute_input":"2021-11-19T06:21:52.39567Z","iopub.status.idle":"2021-11-19T06:21:52.408079Z","shell.execute_reply.started":"2021-11-19T06:21:52.39563Z","shell.execute_reply":"2021-11-19T06:21:52.407398Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SatoriusDataset(Dataset): \n    def __init__(self, csv_file, image_dir, transform=None, resize=None):\n        self.transform = transform\n        self.csv = csv_file \n        self.resize = True if resize else False \n        \n        # resize is the factor, e.g 1.2, 1.3, ... \n        if self.resize: \n            self.height = int(height * resize)\n            self.width = int(width * resize)\n        else: \n            self.height = height\n            self.width = width\n        \n        # collections.defaultdict returns an empty dict when the key is not found\n        self.image_info = collections.defaultdict(dict)\n        \n        # group by id then cell, and get the annotations \n        csv_groupby = self.csv.groupby(['id', 'cell_type'])['annotation'].agg(lambda x: list(x)).reset_index()\n        \n        \n        # image_info is the replacement of csv \n        # use image_info to iterate\n        for index, row in csv_groupby.iterrows(): \n            self.image_info[index] = { \n                'image_id': row['id'],\n                'image_path': os.path.join(image_dir, row['id'] + '.png'),\n                'annotations': row['annotation'],\n                'cell_type': cell_dict[row['cell_type']],\n            }        \n        \n    def __len__(self): \n        return len(self.image_info)\n\n    def get_boxes(self, mask):\n        # the size of the mask is [w, h] and the values are True and False\n        # using np.where will return the location of the True value in the mask \n        mask = np.where(mask)\n\n        # locations of the bounding box \n        x_min = np.min(mask[1])\n        x_max = np.max(mask[1])\n        y_min = np.min(mask[0])\n        y_max = np.max(mask[0])\n\n        return [x_min, y_min, x_max, y_max]\n\n    def __getitem__(self, idx): \n        item = self.image_info[idx]\n\n        image = cv2.imread(item['image_path'], cv2.IMREAD_COLOR)        \n        if self.resize: \n            image = cv2.resize(image, (self.width, self.height))\n        \n        # height goes before width because of rle_decode()\n        masks = np.zeros((len(item['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n\n        for i, mask in enumerate(item['annotations']): \n            mask = rle_decode(mask, (height, width))\n\n            if self.resize: \n                mask = cv2.resize(mask, (self.width, self.height))\n                \n            # the mask only contains true and false, which are 1 and 0\n            mask = np.array(mask) > 0\n            masks[i,:,:] = mask\n            boxes.append(self.get_boxes(mask))\n\n        labels = [item['cell_type'] for _ in range(len(item['annotations']))]\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        \n        # targets must have for the mask-rcnn\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"masks\": masks,\n        }\n\n        if self.transform: \n            image, target = self.transform(image, target)\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:52.409995Z","iopub.execute_input":"2021-11-19T06:21:52.41054Z","iopub.status.idle":"2021-11-19T06:21:52.428501Z","shell.execute_reply.started":"2021-11-19T06:21:52.410505Z","shell.execute_reply":"2021-11-19T06:21:52.427694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the dataset\n\ntest_dataset_csv = pd.read_csv(\"../input/sartorius-cell-instance-segmentation/train.csv\")\ntest_dataset = SatoriusDataset(test_dataset_csv, \"../input/sartorius-cell-instance-segmentation/train\")\n\ntest = test_dataset.__getitem__(1)\nimg = test[0]\ntarget = test[1]\n\nboxes = target['boxes']\nmasks = target['masks']\nlabels = target['labels']\n\nprint(f\"img  :  {img.shape}\")\nprint(f\"boxes:  {boxes.shape}\")\nprint(f\"labels: {labels.shape}\")\nprint(f\"masks:  {masks.shape}\")\n\nprint(\"labels:   \", labels.unique())\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\nax[0].imshow(img)\nax[1].imshow(combine_masks(masks, 0))\n\nfor box in boxes:\n    # box[1] - box[3] because y-axis is reversed\n    ax[1].add_patch(patches.Rectangle((box[0], box[3]), box[2] - box[0], box[1] - box[3], linewidth=1, edgecolor='r', facecolor='none'))\n    \nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:29:36.296944Z","iopub.execute_input":"2021-11-19T06:29:36.297578Z","iopub.status.idle":"2021-11-19T06:29:37.318562Z","shell.execute_reply.started":"2021-11-19T06:29:36.297539Z","shell.execute_reply":"2021-11-19T06:29:37.317866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Model ","metadata":{}},{"cell_type":"code","source":"class MaskRCNN(nn.Module):\n    def __init__(self, num_classes, pretrained_model=None):\n        super().__init__()\n        \n        self.pretrained_model = pretrained_model\n        \n        if NORMALIZE: \n            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                      box_detections_per_img=num_box_detections, \n                                                                      image_mean=RESNET_MEAN, \n                                                                      image_std=RESNET_STD)\n        else:\n            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                      box_detections_per_img=num_box_detections)\n\n        hidden_layer = 256\n        \n        # get the input features of the box_predictor \n        # replace the pretrained box_predictor layer with new FastRCNN\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n        \n        # get the input features of the mask_predictor \n        # replace the pretrained mask_predictor layer with new MaskRCNN\n        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n        self.model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n                \n    def forward(self, x, y=None): \n        # in the training mode, the model needs both x and y (y is the target)\n        # in the val mode, the model only needs x\n        if y: \n            return self.model(x, y)\n        \n        return self.model(x)\n    \n#     def init_model(self):\n#         if self.pretrained_model:\n#             self.model.load_state_dict(self.pretrained_model, map_location=device)\n        \n#         print(\"Loaded pretrained weights\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:21:56.511746Z","iopub.execute_input":"2021-11-19T06:21:56.512017Z","iopub.status.idle":"2021-11-19T06:21:56.522846Z","shell.execute_reply.started":"2021-11-19T06:21:56.511983Z","shell.execute_reply":"2021-11-19T06:21:56.52159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, optimizer, device, scheduler):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = device\n        \n        self.train_loss_mask = []\n        self.train_loss_classifier = []\n        self.train_loss = []\n        self.val_loss_mask = []\n        self.val_loss_classifier = []\n        self.val_loss = []\n        \n    def fit(self, train_loader, val_loader, n_epochs):\n        for epoch in range(n_epochs): \n            \n            # train and val\n            self.train_epoch(train_loader)            \n            self.val_epoch(val_loader)\n            \n            if self.scheduler: \n                self.scheduler.step() \n            \n            print(f\"epoch {epoch + 1} train:  mask loss : {self.train_loss_mask[-1]:3f}, classifier loss:  {self.train_loss_classifier[-1]:3f}\")\n            print(f\"epoch {epoch + 1} val:    mask loss : {self.val_loss_mask[-1]:3f},   classifier loss: {self.val_loss_classifier[-1]:3f}\")\n            print(f\"epoch {epoch + 1}:        train loss: {self.train_loss[-1]:3f},      val_loss       : {self.val_loss[-1]:3f}\")\n\n            print('save model ...')\n            torch.save(self.model.state_dict(), f'model-e{epoch}.pt')\n\n            print('\\n\\n')\n                                \n    def train_epoch(self, train_loader):\n        self.model.train()\n        self.train_loss_mask.append(0)\n        self.train_loss_classifier.append(0)\n        self.train_loss.append(0)\n\n        for i, data in tqdm(enumerate(train_loader, 0)): \n            x, y = data\n            x = list(img.to(self.device) for img in x)\n            y = [{k: v.to(self.device) for k, v in t.items()} for t in y]\n            \n            self.optimizer.zero_grad()\n            \n            # in training mode, the model return the loss\n            # in val mode, the model return the predictions\n            losses = self.model(x, y)\n            loss = sum(loss for loss in losses.values())\n            loss.backward()\n            self.optimizer.step()\n            \n            self.train_loss_mask[-1] += losses['loss_mask'].item()\n            self.train_loss_classifier[-1] += losses['loss_classifier'].item()\n            self.train_loss[-1] += loss.item()\n        \n        # gets the average\n        self.train_loss_mask[-1] /= len(train_loader)\n        self.train_loss_classifier[-1] /= len(train_loader)\n        self.train_loss[-1] /= len(train_loader)\n        \n    def val_epoch(self, val_loader):\n        self.val_loss_mask.append(0)\n        self.val_loss_classifier.append(0)\n        self.val_loss.append(0)\n\n        for i, data in tqdm(enumerate(val_loader, 0)): \n            with torch.no_grad():\n                x, y = data \n                x = list(img.to(self.device) for img in x)\n                y = [{k: v.to(self.device) for k, v in t.items()} for t in y]\n                \n                # in training mode, the model return the loss\n                # in val mode, the model return the predictions\n                losses = model(x, y)\n                loss = sum(loss for loss in losses.values())\n                \n                self.val_loss_mask[-1] += losses['loss_mask'].item()\n                self.val_loss_classifier[-1] += losses['loss_classifier'].item()\n                self.val_loss[-1] += loss.item()\n\n        # gets the average\n        self.val_loss_mask[-1] /= len(val_loader)\n        self.val_loss_classifier[-1] /= len(val_loader)\n        self.val_loss[-1] /= len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T06:22:01.448333Z","iopub.execute_input":"2021-11-19T06:22:01.448803Z","iopub.status.idle":"2021-11-19T06:22:01.481113Z","shell.execute_reply.started":"2021-11-19T06:22:01.448765Z","shell.execute_reply":"2021-11-19T06:22:01.480453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = get_transform(False)\n\ntrain_csv = pd.read_csv(\"../input/sartorius-cell-instance-segmentation/train.csv\")\n\n# the layout of the df_images would be: id, cell_type, annotation\n# there would be one row for one image because each image only contains one cell_type\ndf_images = train_csv.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n\ntrain_df, test_df = train_test_split(df_images, stratify=df_images['cell_type'], \n                                                  test_size=0.2,\n                                                  random_state=1702)\n\n# dataloader for training\ntrain_df = train_csv[train_csv['id'].isin(train_df['id'])]\ntrain_dataset = SatoriusDataset(train_df, \"../input/sartorius-cell-instance-segmentation/train\", transform=transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                              num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n\n# dataloader for validation\ntest_df = train_csv[train_csv['id'].isin(test_df['id'])]\ntest_dataset = SatoriusDataset(test_df, \"../input/sartorius-cell-instance-segmentation/train\", transform=transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True,\n                             num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MaskRCNN(len(cell_dict))\nmodel.to(device)\n\nfor param in model.parameters():\n    param.requires_grad = True\n    \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\ntrainer = Trainer(model, optimizer, device, lr_scheduler)\ntrainer.fit(train_dataloader, test_dataloader, n_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T03:39:37.842734Z","iopub.execute_input":"2021-11-19T03:39:37.843261Z","iopub.status.idle":"2021-11-19T03:39:46.073187Z","shell.execute_reply.started":"2021-11-19T03:39:37.843225Z","shell.execute_reply":"2021-11-19T03:39:46.072135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to-do gets the best model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to-do: include the iou score for the prediction's image\n\ntest_dataset_temp = SatoriusDataset(test_df, \"../input/sartorius-cell-instance-segmentation/train\")\n\nbest_model = \"../input/satorius-models/model-e9.pt\"\nnum_rows = 3\nfig, ax = plt.subplots(num_rows * batch_size, 3, figsize=(15, 20))\n\nmodel = MaskRCNN(len(cell_dict))\nmodel.to(device)\n\ncheckpoint = torch.load(best_model)\nmodel.load_state_dict(checkpoint)\nmodel.eval()\n\nfor i, data in enumerate(test_dataloader): \n    x, y = data\n    x = list(img.to(device) for img in x)\n    \n    with torch.no_grad():\n            predictions = model(x, None)\n    \n    for j in range(batch_size):\n        original_img = test_dataset_temp.__getitem__(i * 2 + j)[0]\n        \n        ax[i * 2 + j, 0].imshow(original_img)\n        ax[i * 2 + j, 1].imshow(combine_masks(y[j][\"masks\"], 0))\n        ax[i * 2 + j, 2].imshow(combine_masks(predictions[j][\"masks\"].squeeze().cpu().numpy(), 0))\n        \n        ax[i * 2 + j, 0].axis('off')\n        ax[i * 2 + j, 1].axis('off')\n        ax[i * 2 + j, 2].axis('off')\n\n        ax[i * 2 + j, 1].title.set_text(f'num_cells: {len(y[j][\"masks\"])}')\n        ax[i * 2 + j, 2].title.set_text(f'num_cells: {len(predictions[j][\"masks\"].squeeze().cpu().numpy())}')\n        \n    if i == num_rows - 1: \n        break","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:14:48.244830Z","iopub.execute_input":"2021-11-19T07:14:48.245576Z","iopub.status.idle":"2021-11-19T07:14:57.145450Z","shell.execute_reply.started":"2021-11-19T07:14:48.245537Z","shell.execute_reply":"2021-11-19T07:14:57.142198Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# to-do: predict","metadata":{},"execution_count":null,"outputs":[]}]}